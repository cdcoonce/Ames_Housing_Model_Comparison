# ğŸ  Ames Housing Price Prediction â€” Baselines vs Neural Network

**Author:** Charles Coonce  
**Course:** DAT 402 â€” Applied Predictive Modeling  
**Date:** October 2025  

---

## ğŸ¯ Project Overview

This project analyzes the **Ames Housing Dataset**, a well-known dataset for regression tasks in predictive modeling.  
The goal was to **compare three models** â€” Linear Regression, k-Nearest Neighbors (kNN), and a Neural Network â€” to understand the **biasâ€“variance tradeoff** and observe how model flexibility impacts predictive performance.

---

## ğŸ“Š Objectives

- Build and evaluate **Linear Regression** and **kNN** baselines  
- Demonstrate **biasâ€“variance tradeoff** and **cross-validation**  
- Develop a **Neural Network** using TensorFlow/Keras  
- Compare model accuracy using RMSE, MAE, and RÂ² metrics  
- Export results and analysis to an interactive HTML notebook  

---

## ğŸ“¦ Dataset

**Source:** [Kaggle â€” Ames Housing Dataset](https://www.kaggle.com/datasets/shashanknecrothapa/ames-housing-dataset)

- **Instances:** 2,930 homes  
- **Features:** 79 (numerical + categorical)  
- **Target Variable:** `SalePrice`  
- **Key Engineered Features:**
  - `TotalSF` â€” total living area (basement + first + second floor)  
  - `TotalBath` â€” combined count of full and half baths  
  - `AgeAtSale` â€” house age when sold  
  - `RemodelAge` â€” time since last remodel  

---

## ğŸ§° Tools & Libraries

| Category | Libraries Used |
|-----------|----------------|
| Data Handling | `pandas`, `numpy`, `matplotlib` |
| Modeling | `scikit-learn` (`LinearRegression`, `KNeighborsRegressor`) |
| Neural Network | `TensorFlow`, `Keras` |
| Preprocessing | `ColumnTransformer`, `Pipeline`, `StandardScaler`, `OneHotEncoder` |
| Validation & Metrics | `train_test_split`, `KFold`, `cross_val_score`, `RMSE`, `MAE`, `RÂ²` |

---

## âš™ï¸ Methodology

1. **Data Preparation**
   - Removed irrelevant columns, handled missing data.
   - Selected top correlated features with `SalePrice`.
   - Added engineered features to improve interpretability.

2. **Preprocessing Pipeline**
   - Imputation for numeric and categorical features.
   - Standard scaling for numeric data.
   - One-hot encoding for categorical variables.

3. **Model Training**
   - Linear Regression: baseline with high bias, low variance.
   - kNN: explored `k = 1â€“40` to demonstrate biasâ€“variance tradeoff.
   - Neural Network: two hidden layers (128 â†’ 64 ReLU), linear output.

4. **Evaluation Metrics**
   - **RMSE** â€” Root Mean Squared Error  
   - **MAE** â€” Mean Absolute Error  
   - **RÂ²** â€” Coefficient of Determination  

---

## ğŸ“ˆ Results Summary

| Model | RMSE | MAE | Notes |
|--------|------|-----|-------|
| **Linear Regression** | 28,152 | 19,184 | Simple, interpretable baseline |
| **kNN (k=5)** | 27,025 | 18,276 | Improved accuracy, more flexible |
| **Neural Network** | **26,940** | **18,451** | Best performance; captured complex relationships |

- The **Neural Network** achieved the lowest overall error and best generalization.  
- **kNN** demonstrated a clear biasâ€“variance tradeoff.  
- **Linear Regression** remained useful as a transparent baseline.

---

## ğŸ§  Key Takeaways

- Increasing model flexibility (from Linear â†’ kNN â†’ NN) **reduced bias and improved accuracy**.
- The **Neural Network** achieved balanced performance without significant overfitting.
- **Cross-validation** confirmed kNNâ€™s advantage over Linear Regression.
- Proper preprocessing and feature engineering significantly enhanced results.

---

## ğŸª„ Visualizations

- **Correlation heatmap** to identify predictive features  
- **kNN biasâ€“variance plot** showing RMSE vs. neighbors  
- **Neural network training curves** (RMSE & MAE over epochs)  
- **Prediction parity plot** â€” predicted vs. actual prices  
*(All plots are viewable in the accompanying Jupyter Notebook or HTML export.)*

---

## ğŸ’¡ Future Improvements

- Perform **hyperparameter tuning** for the Neural Network  
- Add **dropout** or **regularization** to reduce potential overfitting  
- Experiment with **ensemble methods** like Random Forest or XGBoost  
- Extend analysis with **feature importance** and **SHAP values**  

---

## ğŸ§© Repository Structure

```text

Ames_Housing_Price_Prediction/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ AmesHousing.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ AmesHousing_Project2.ipynb
â”‚
â”œâ”€â”€ exports/
â”‚   â””â”€â”€ AmesHousing_Project2.html
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸš€ How to Run (Using uv)

```bash
# 1ï¸âƒ£ Clone the repository
git clone https://github.com/cdcoonce/Ames_Housing_Model_Comparison.git
cd Ames_Housing_Model_Comparison

# 2ï¸âƒ£ Create a uv-managed virtual environment (Python 3.11 recommended)
uv venv --python 3.11
source .venv/bin/activate

# 3ï¸âƒ£ Install project dependencies
uv sync
```

---

**ğŸ“¬ Author:**  
*Charles Coonce*  
[Portfolio Website](https://charleslikesdata.com) Â· [GitHub](https://github.com/cdcoonce)
